{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg(n_episodes=1000, print_every=100):\n",
    "    # loop from n_episodes\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     \n",
    "        states = env_info.vector_observations\n",
    "        agent.reset()\n",
    "        score = np.zeros(num_agents)\n",
    "        while True:\n",
    "            actions = agent.act(states)\n",
    "            env_info = env.step(actions)[brain_name]        \n",
    "            next_states = env_info.vector_observations   # Get the next states for each unity agent in the environment\n",
    "            rewards = env_info.rewards                   # Get the rewards for each the Unity ML-agent in the environment\n",
    "            dones = env_info.local_done                  # See if episode has finished for each the Unity ML-agent in the environment\n",
    "            agent.step(states, actions, rewards, next_states, dones)\n",
    "            states = next_states                 # Set new states to current states for determining next actions\n",
    "            score += rewards         # Update episode score for each the Unity ML-agent\n",
    "            if np.any(dones):\n",
    "                break\n",
    "\n",
    "        scores.append(np.mean(score))\n",
    "        average_score = np.mean(scores[i_episode-min(i_episode,average_score_range):i_episode+1])\n",
    "        print('\\nEpisode {}\\tEpisode Score: {:.3f}\\tAverage Score: {:.3f}'.format(i_episode, scores[i_episode-1], average_score), end=\"\")\n",
    "\n",
    "        actor_filename = \"actor.pth\"\n",
    "        torch.save(agent.actor_local.state_dict(), actor_filename)\n",
    "        critic_filename = \"critic.pth\"\n",
    "        torch.save(agent.critic_local.state_dict(), critic_filename)\n",
    "\n",
    "        if i_episode > 100 and average_score >= solved_score:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.3f}'.format(i_episode, average_score))\n",
    "            break\n",
    "    return scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This is a Deep Deterministic Policy Gradient (DDPG) function implementation for training an agent in a Unity ML-Agents environment. It takes several hyperparameters as input, including the number of episodes to train on (n_episodes), The frequency with which to print the current and average score during the training (print_every), The size of the window over which to compute the average score (average_score_range).\n",
    "\n",
    "## Hyperparamters\n",
    "\n",
    "\n",
    "\n",
    "The function implements the following steps for each episode of training:\n",
    "\n",
    "Resets the environment to get the initial state.\n",
    "Resets the agent for the new episode and sets the initial episode score to zero.\n",
    "Runs the episode training loop; takes an action as a function of the current state observations and updates the agents' Actor and Critic networks based on the next state and reward received.\n",
    "If the environment episode is done, the loop is exited; otherwise, the loop continues until done == true.\n",
    "Calculates the episode score and prints the current and average score.\n",
    "Saves the checkpoint trained Actor and Critic network weights after each episode.\n",
    "Checks to see if the task is solved by verifying whether the average score over the last 100 episodes is greater than or equal to the solved score. If the task is solved, the training is stopped, and the network weights and scores are saved.\n",
    "The function returns a list of episode scores."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work\n",
    "\n",
    "Here are a few possible directions for future work on DDPG:\n",
    "\n",
    "Improving exploration: Exploration is a crucial component of reinforcement learning, and DDPG can sometimes get stuck in local optima. One way to improve exploration is to use a more sophisticated exploration strategy, such as adding noise to the actions or using parameter space noise. Another approach is to use a different algorithm that is better suited for exploration, such as Soft Actor-Critic (SAC).\n",
    "\n",
    "Incorporating hierarchical learning: Hierarchical learning is a way to structure the learning process such that the agent can learn to solve complex tasks by breaking them down into smaller sub-tasks. DDPG could be extended to incorporate hierarchical learning by adding a hierarchy of policies that learn to solve sub-tasks.\n",
    "\n",
    "Using ensembles of policies: Ensembles of policies can improve the stability and robustness of reinforcement learning algorithms. DDPG could be extended to use an ensemble of policies, where each policy is trained on a different subset of the environment or with different hyperparameters.\n",
    "\n",
    "Adapting to changing environments: In many real-world scenarios, the environment can change over time. DDPG could be extended to learn to adapt to changes in the environment, for example by using meta-learning or by incorporating online learning.\n",
    "\n",
    "Scaling to large environments: DDPG can struggle with large state and action spaces. One way to scale DDPG to large environments is to use function approximation techniques such as deep neural networks. Another approach is to use techniques such as the successor features method to reduce the dimensionality of the state space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "0e93ad9896474b18971f12a128f2bcea180c361d3b8f447a057d728ac9f297b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
